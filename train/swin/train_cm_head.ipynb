{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Training for Contact-Map Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames_per_seq : int = 8\n",
    "emb_dim : int = 768\n",
    "predictor_input_size = emb_dim + n_frames_per_seq*42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter # pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_root = '../data/h2o/'\n",
    "sample_root_train = h2o_root + f'seq_{n_frames_per_seq}_train/'\n",
    "sample_root_val = h2o_root + f'seq_{n_frames_per_seq}_val/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTrain(torch.utils.data.DataLoader):\n",
    "    def __init__(self, mode, h2o_dir, sample_dir):\n",
    "        assert mode in ['train', 'val'], f'Invalid mode {mode}. Expected: train, val'\n",
    "        self.sample_dir = sample_dir\n",
    "        self.emb_dir = sample_dir + f'emb_swin_{mode}/'\n",
    "        # self.dist_dir = sample_dir + f'distances_{mode}/'\n",
    "        self.dist_dir = sample_dir + f'cm_{mode}/'\n",
    "        self.labels = np.load(h2o_dir + f'action_labels_{mode}.npy')\n",
    "        self.n_actions = self.labels.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_actions\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        emb = np.load(self.emb_dir + f'{idx+1}.npy')\n",
    "        dist = np.load(self.dist_dir + f'{(idx+1):03d}.npy').flatten()\n",
    "        return np.hstack([emb, dist]).astype(np.float32), self.labels[idx]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 36 action labels (1-36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define MLP predictor model\n",
    "class ActionPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 36)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(F.relu(x))\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 120\n",
    "lr = 0.001\n",
    "momentum = 0.98\n",
    "print_every_iters = 230\n",
    "weight_dir = 'weights/cmhead/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "data_train = DataTrain('train', h2o_root, sample_root_train)\n",
    "data_val = DataTrain('val', h2o_root, sample_root_val)\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=2, shuffle=False, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=2, shuffle=False, num_workers=4)\n",
    "\n",
    "# model\n",
    "model = ActionPredictor(predictor_input_size)\n",
    "model.train()\n",
    "\n",
    "# define optimizer and loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# tensorboard\n",
    "tb_writer = SummaryWriter('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 121 ====================\n",
      "[Epoch: 121 / 120, Iter:   230 / 285] Training loss: 2.640\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 122 ====================\n",
      "[Epoch: 122 / 120, Iter:   230 / 285] Training loss: 2.640\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 123 ====================\n",
      "[Epoch: 123 / 120, Iter:   230 / 285] Training loss: 2.640\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 124 ====================\n",
      "[Epoch: 124 / 120, Iter:   230 / 285] Training loss: 2.640\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 125 ====================\n",
      "[Epoch: 125 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 126 ====================\n",
      "[Epoch: 126 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 127 ====================\n",
      "[Epoch: 127 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 128 ====================\n",
      "[Epoch: 128 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 129 ====================\n",
      "[Epoch: 129 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 130 ====================\n",
      "[Epoch: 130 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 131 ====================\n",
      "[Epoch: 131 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 132 ====================\n",
      "[Epoch: 132 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.639\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 133 ====================\n",
      "[Epoch: 133 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 134 ====================\n",
      "[Epoch: 134 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 135 ====================\n",
      "[Epoch: 135 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 136 ====================\n",
      "[Epoch: 136 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 137 ====================\n",
      "[Epoch: 137 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.787\n",
      "==================== Epoch 138 ====================\n",
      "[Epoch: 138 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 139 ====================\n",
      "[Epoch: 139 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 140 ====================\n",
      "[Epoch: 140 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 141 ====================\n",
      "[Epoch: 141 / 120, Iter:   230 / 285] Training loss: 2.639\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 142 ====================\n",
      "[Epoch: 142 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 143 ====================\n",
      "[Epoch: 143 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 144 ====================\n",
      "[Epoch: 144 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 145 ====================\n",
      "[Epoch: 145 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 146 ====================\n",
      "[Epoch: 146 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 147 ====================\n",
      "[Epoch: 147 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 148 ====================\n",
      "[Epoch: 148 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.795\n",
      "==================== Epoch 149 ====================\n",
      "[Epoch: 149 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 150 ====================\n",
      "[Epoch: 150 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 151 ====================\n",
      "[Epoch: 151 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 152 ====================\n",
      "[Epoch: 152 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 153 ====================\n",
      "[Epoch: 153 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 154 ====================\n",
      "[Epoch: 154 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.638\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 155 ====================\n",
      "[Epoch: 155 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.637\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 156 ====================\n",
      "[Epoch: 156 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.637\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 157 ====================\n",
      "[Epoch: 157 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.637\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 158 ====================\n",
      "[Epoch: 158 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.637\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 159 ====================\n",
      "[Epoch: 159 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.637\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "==================== Epoch 160 ====================\n",
      "[Epoch: 160 / 120, Iter:   230 / 285] Training loss: 2.638\n",
      "Training Loss: 2.637\n",
      "Validation Loss: 2.894\n",
      "Validation Acc: 0.803\n",
      "Successfully trained 120 epochs.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(120, 160):\n",
    "    print(20*'=', 'Epoch %d' % (epoch + 1), 20*'=')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X) # shape (1,36) or (2,36)\n",
    "        # outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print batch statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % print_every_iters == 0:\n",
    "            print(\n",
    "                f'[Epoch: {epoch + 1} / {n_epochs},'\n",
    "                f' Iter: {i + 1:5d} / {len(train_loader)}]'\n",
    "                f' Training loss: {running_loss / (i + 1):.3f}'\n",
    "            )\n",
    "\n",
    "    # epoch statistics\n",
    "    mean_loss = running_loss / len(train_loader)\n",
    "    tb_writer.add_scalar('Training Loss', mean_loss, epoch)\n",
    "    print('Training Loss: %.3f' % (mean_loss))\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total, correct = 0, 0\n",
    "    for (X, y) in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X)\n",
    "            # outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, y)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # make prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    mean_loss = running_loss / len(val_loader)\n",
    "    tb_writer.add_scalar('Validation Loss', mean_loss, epoch)\n",
    "    tb_writer.add_scalar('Validation Accuracy', accuracy, epoch)\n",
    "    tb_writer.flush()\n",
    "    print('Validation Loss: %.3f\\nValidation Acc: %.3f' % (mean_loss, accuracy))\n",
    "\n",
    "    # save weights of current epoch\n",
    "    torch.save(model.state_dict(), f'{weight_dir}e_{epoch}.pt')\n",
    "\n",
    "tb_writer.close()\n",
    "print(f'Successfully trained {n_epochs} epochs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('...'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
